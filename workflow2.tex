\documentclass{report}
\usepackage[landscape]{geometry}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage[inline]{enumitem}
\usepackage{tikz}
\usetikzlibrary{calc,shapes, positioning,arrows}

\usepackage{bibleref}

\usepackage{minted1}
%\usepackage{minted}
\usepackage{listings}
\newcommand{\mi}[1]{\lstinline{#1}}

\makeatletter
\newenvironment{python}{%
  \VerbatimEnvironment
  \minted@resetoptions
  \setkeys{minted@opt}{}
      \begin{VerbatimOut}{\jobname.pyg}}
{%
      \end{VerbatimOut}
      \minted@pygmentize{python}
      \DeleteFile{\jobname.pyg}}
\makeatother

\usepackage{multicol}

%\usepackage[usenames, dvipsnames]{color}
%\newcommand{\fixme}[1]{{#1}}

\title{Work flow with gcdata.sqlite and several Python scripts}
\author{Grietje Commelin}

\begin{document}
\maketitle

\noindent
In this document I try to explain the work flow that I follow to gather, improve and use data about participant references by running Python scripts on the ETCBC database and my own gcdata.sqlite.\\
Data is indicated with red frames, and scripts with blue ones.

\begin{tikzpicture}[outline/.style={draw=#1,thick,fill=#1!50},
>=stealth',thick,black!50,text=black,
every new ->/.style={shorten >=.3cm}]
\centering

\node [outline=red] (input) {ETCBC4 data};
\node [outline=blue](part1) [below = of input]{Part-1.py};
\draw [shorten > =.1cm, ->] (input.south) -- (part1.north);
\node[outline=blue](qf) [below right = of input]{DSU+QF.py};
\draw [shorten > =.3cm, ->] (input.south) -- (qf.north);
\node[dashed, outline=blue, fill opacity =.5](enc) [below left = of input]{encoding\_options.py};
\draw [dashed, shorten > =.3cm, ->] (input.south) -- (enc.north);

\node [outline=red] (pt) [below=of part1]{gcdata.pt};
\draw [shorten > =.1cm, ->] (part1) -- (pt);

\node [dashed, outline=red, fill opacity =.5](enc2)[below = of enc]{gcdata.enc};
\draw [dashed, shorten > =.1cm, ->] (enc.south) -- (enc2.north);
\draw [dashed, shorten > =.1cm, ->] (enc2.east) -- (part1.west);

\node [outline=red] (gcdsu) [below=of qf]{gcdata.gcdsu};
\draw [shorten > =.1cm, ->] (qf.south) -- (gcdsu.north);
\node [outline=blue] (part2) [below = of pt] {Part-2.py};
\draw [shorten > =.1cm, ->] (pt) to [bend right = 20] (part2.north);
\draw [shorten > =.1cm, ->] (part2.north) to [bend right = 20] (pt);
\draw [shorten > =.1cm, ->] (gcdsu.south west) to [bend right = 20] (part2.north east);
\draw [shorten > =.1cm, ->] (part2.north east) to [bend right = 20] (gcdsu.south west);

\node [outline=green](show)[below = of part2]{Display of text with data};
\draw [shorten > =.1cm, ->] (gcdsu) to [bend left = 40] (show);
\draw [shorten > =.1cm, ->] (pt) to [bend right = 60] (show.west);
\draw [dashed, shorten > =.1cm, ->] (show.south) to [bend right = 120] (input);
\end{tikzpicture}

\chapter{Packages and LAF data that have to be loaded for executing the scripts}

\begin{python}
#{{{ Import all important libraries
import os, sys, time
if not '/projects/1fdd6b0b-5199-4b0e-9f7f-7759af4a5e7a/.local/lib/python3.4/site-packages/' in sys.path:
    sys.path.append('/projects/1fdd6b0b-5199-4b0e-9f7f-7759af4a5e7a/.local/lib/python3.4/site-packages/')
import collections
import random
import laf
from copy import deepcopy
from laf.fabric import LafFabric
import etcbc
from etcbc.preprocess import prepare
from etcbc.lib import Transcription, monad_set
from etcbc.trees import Tree
from etcbc.featuredoc import FeatureDoc
fabric = LafFabric()
tr = Transcription()

print("It works so far." )
#}}}
\end{python}

\begin{python}
#{{{ Load the API
API = fabric.load("etcbc4", "--", "feature-doc", {
    "xmlids": {"node": False, "edge": False},
    "features": ('''
            otype
            domain
            function
            g_cons
            g_cons_utf8
            g_lex
            g_lex_utf8
            g_nme
            g_nme_utf8
            g_pfm
            g_pfm_utf8
            g_prs
            g_prs_utf8
            g_uvf
            g_uvf_utf8
            g_vbe
            g_vbe_utf8
            g_vbs
            g_vbs_utf8
            g_word
            g_word_utf8
            gn
            lex
            lex_utf8
            ls
            nme
            nu
            number
            pdp
            pfm
            prs
            ps
            rela
            sp
            st
            tab
            trailer_utf8
            txt
            typ
            uvf
            vbe
            vbs
            vs
            vt
            book
            chapter
            label
            verse
    ''','''
            mother
    '''),
    "primary": False,
    "prepare": prepare,
})
exec(fabric.localnames.format(var='fabric'))

#}}}
\end{python}

\begin{python}
#{{{ Stuff necessary to load parents relationships
type_info = (
    ("word", ''),
    ("subphrase", 'U'),
    ("phrase", 'P'),
    ("clause", 'C'),
    ("sentence", 'S'),
)
type_table = dict(t for t in type_info)
type_order = [t[0] for t in type_info]

pos_table = {
    'adjv': 'aj',
    'advb': 'av',
    'art': 'dt',
    'conj': 'cj',
    'intj': 'ij',
    'inrg': 'ir',
    'nega': 'ng',
    'subs': 'n',
    'nmpr': 'n-pr',
    'prep': 'pp',
    'prps': 'pr-ps',
    'prde': 'pr-dem',
    'prin': 'pr-int',
    'verb': 'vb',
}

ccr_info = {
    'Adju': ('r', 'Cadju'),
    'Attr': ('r', 'Cattr'),
    'Cmpl': ('r', 'Ccmpl'),
    'CoVo': ('n', 'Ccovo'),
    'Coor': ('x', 'Ccoor'),
    'Objc': ('r', 'Cobjc'),
    'PrAd': ('r', 'Cprad'),
    'PreC': ('r', 'Cprec'),
    'Resu': ('n', 'Cresu'),
    'RgRc': ('r', 'Crgrc'),
    'Spec': ('r', 'Cspec'),
    'Subj': ('r', 'Csubj'),
    'NA':   ('n', 'C'),
}

tree_types = ('sentence', 'clause', 'clause_atom', 'phrase', 'subphrase', 'word')
(root_type, leaf_type, clause_type) = (tree_types[0], tree_types[-1], 'clause')
ccr_table = dict((c[0],c[1][1]) for c in ccr_info.items())
ccr_class = dict((c[0],c[1][0]) for c in ccr_info.items())

root_verse = {}
root_clause_atom = {}

for n in NN():
    otype = F.otype.v(n)
    if otype == "book" and F.etcbc4_sft_book.v(n) != "Genesis":
        break
    elif otype == "chapter" and int(F.etcbc4_sft_chapter.v(n)) > 5:
        break
    elif otype == "verse": cur_verse = F.label.v(n)
    elif otype == "clause_atom":
        root_verse[n] = cur_verse
        cur_clause_atom = F.etcbc4_ft_number.v(n)
    elif otype == "word":
        root_verse[n] = cur_verse
        root_clause_atom[n] = cur_clause_atom
#        print(list(C.parent.v(n)))

tree = Tree(API, otypes = tree_types,
     clause_type=clause_type,
     ccr_feature='rela',
     pt_feature='typ',
     pos_feature='sp',
     mother_feature = 'mother',
     )
#tree.restructure_clauses(ccr_class)
results = tree.relations()
parent = results['eparent']
sisters = results['sisters']
children = results['echildren']
elder_sister = results['elder_sister']
print("Ready for processing") # Was msg()
#}}}
\end{python}

\begin{python}
#{{{ Import the helper functions

def give_me_your_daughters(n):
        global F
        daughters = []
        for x in Ci.mother.v(n):
            daughters.append(x)
            if list(Ci.mother.v(x)) != []:
                daughters += give_me_your_daughters(x)
        return(daughters)

def give_me_your_words(n):
        global F
        if F.etcbc4_db_otype.v(n) == 'word':
                return [n]
        else:
            list_of_word_lists = [give_me_your_words(c) for c in children[n]]
            return [item for sublist in list_of_word_lists for item in sublist]

def give_me_your_subphrases(n):
        global F
        subphrases = []
        for x in children[n]:
            if F.etcbc4_db_otype.v(x) == "subphrase":
                subphrases.append(x)
            if children[x] != []:
                subphrases += give_me_your_subphrases(x)
        return subphrases

def get_text_of_word_list(word_list):
        global F
        return ' '.join(map(F.etcbc4_ft_g_cons_utf8.v, word_list))

def get_cons_of_word_list(word_list):
        global F
        return ' '.join(map(F.etcbc4_ft_g_cons.v, word_list))

def give_me_your_parents_up_to(node, parent_type='clause', limit=19):
    # This function is very dangerous. If you feed it something larger than a clause, stuff explodes.
        global parent
        if limit < 0:
                return []
        if not node in parent:
                return ["not in parent"]
        p = parent[node]
        if F.etcbc4_db_otype.v(p) == parent_type:
                return [p]
        else:
                return [p] + give_me_your_parents_up_to(p, parent_type, limit-1)

suffix_dict = {
'W'   : { 'ps' : 'p3', 'nu' : 'sg', 'gn' : 'm' },
'K'   : { 'ps' : 'p2', 'nu' : 'sg', 'gn' : 'm' },
'J'   : { 'ps' : 'p1', 'nu' : 'sg', 'gn' : 'unknown' },
'M'   : { 'ps' : 'p3', 'nu' : 'pl', 'gn' : 'm' },
'H'   : { 'ps' : 'p3', 'nu' : 'sg', 'gn' : 'f' },
'HM'  : { 'ps' : 'p3', 'nu' : 'pl', 'gn' : 'm' },
'KM'  : { 'ps' : 'p2', 'nu' : 'pl', 'gn' : 'm' },
'NW'  : { 'ps' : 'p1', 'nu' : 'pl', 'gn' : 'unknown' },
'HW'  : { 'ps' : 'p3', 'nu' : 'sg', 'gn' : 'm' },
'NJ'  : { 'ps' : 'p1', 'nu' : 'sg', 'gn' : 'unknown' },
'K='  : { 'ps' : 'p2', 'nu' : 'sg', 'gn' : 'f' },
'HN'  : { 'ps' : 'p3', 'nu' : 'pl', 'gn' : 'f' },
'H='  : { 'ps' : 'p3', 'nu' : 'sg', 'gn' : 'unknown' },
'MW'  : { 'ps' : 'p3', 'nu' : 'pl', 'gn' : 'm' },
'N'   : { 'ps' : 'p3', 'nu' : 'pl', 'gn' : 'f' },
'KN'  : { 'ps' : 'p2', 'nu' : 'pl', 'gn' : 'f' },
}

def insert_dict_in_db(cursor, table, values):
        columns = ', '.join(values.keys())
        placeholders = ':'+', :'.join(values.keys())
        query = 'INSERT INTO %s (%s) VALUES (%s)' % (table, columns, placeholders)
#        print(query)
        cursor.execute(query, values)

#}}}
\end{python}

\chapter{DSU+QF.py}
\lstset{language=python,basicstyle=\ttfamily}

\begin{multicols}{2}
In this task I want to extract information from the ETCBC database + ask for user-input about DSU's and QF's + put it into an SQLite-table.
A DSU is a Direct Speech Unit, and a QF is a Quotative Frame.

\textbf{WARNING:} DSU's are found on the basis of \mi{text_type}s of \mi{clause_atom}s as stored in the ETCBC database. However, testing shows that these are not 100\% correct and will need improvement.

\textbf{WARNING2:} This task assumes that a DSU ends when the embedding (number of \mi{Q}'s in the \mi{text_type} of a \mi{clause_atom}) decreases.
However, it might very well be possible that various DSU's follow each other on the same level of embedding without intervening lower-\mi{Q} \mi{clause_atom}s.
At the moment these cases will not be detected by this script!
\end{multicols}

\begin{python}
#{{{ Create an SQLite-table with a row for every QF + DSU, and columns for useful information about these DSU's.
import sqlite3
db = sqlite3.connect('gcdata.sqlite')
cursor = db.cursor()

create_table_query = open('gcdsu.sql').read()

cursor.execute(create_table_query)

db.commit()
#}}}
\end{python}

\begin{multicols}{2}
Now we want to determine which \mi{clause_atom_number}s constitute QF's, and gather \mi{user_input} about them.
\begin{itemize}
 \item Show 4 clauses before DSU + first 4 clauses of DSU itself, all with \mi{clause_atom_number} and \mi{g_consonants}.
 \item Ask for block of \mi{clause_atom}s before DSU:
 \begin{itemize}
    \item Which \mi{clause_atom}s (if any) are QF
    \item What ``MySp" is. Can be any name
    \item What ``QFSp" is. Can be anything, or \mi{NULL}
    \item What ``QFSptype" is. Options: \mi{Noun}, \mi{Noun_phrase}, \mi{Name}, \mi{Name_phrase}, \mi{Pers_pronoun}, \mi{Suffix}, \mi{Verb}
    \item What ``MyAd" is. Can be any name
    \item What ``QFAd" is. Can be anything, or \mi{NULL}
    \item What ``QFAdtype" is. Options: \mi{Noun}, \mi{Noun_phrase}, \mi{Name}, \mi{Name_phrase}, \mi{Pers_pronoun}, \mi{Suffix}, \mi{Verb}
    \item What ``QFAdprep" is
    \item What ``QFplus" is. Can be anything, or \mi{NULL}
    \item What ``QFplustype" is. Can be something like \mi{Time} or \mi{Space}, or \mi{NULL}
  \end{itemize}
\end{itemize}
\end{multicols}

\begin{python}
#{{{ Verbs
def verbs(QFClANrs):
    QFverb,QFverbspeech,QFLMR = [],[],"No"
    for x in QFClANrs:
        for y in get_text_of_word_list(give_me_your_words(x)):
            if F.etcbc4_ft_sp.v(y) == "verb":
                QFverb.append(y)
                if F.etcbc4_ft_ls.v(y) == "quot":
                    QFverbspeech.append(y)
                    if F.etcbc4_ft_lex.v(y) == ">MR[" and F.etcbc4_ft_vt.v(y) == "infc":
                        QFLMR = "Yes"
    return(','.join(str(x) for x in QFverb),','.join(str(x) for x in QFverbspeech),QFLMR)

#}}}
\end{python}

\begin{python}
#{{{ Process_DSU
def process_DSU(ClANrs, potential_QF, place,prev_context,context,level):
    DSUdata = {}
    print(place)
    print("Potential QF:")  # Show the clause_atom_numbers of potential_QF with their text
    for x in potential_QF:
        print('{0:<8} {1:>35} {2} {3:<35}'.format(x, get_text_of_word_list(give_me_your_words(x))," | ", get_cons_of_word_list(give_me_your_words(x))))
    print("Start of DSU:")  # Show the first clause_atom_numbers and text of the DSU
    for y in ClANrs[:3]:
        print('{0:<8} {1:>35} {2} {3:<35}'.format(y, get_text_of_word_list(give_me_your_words(y))," | ",get_cons_of_word_list(give_me_your_words(y))))
    print("-"*20)
    DSUdata["a"] = place
    DSUdata["Level"] = level
#    DSUdata["ClANrs"] = ','.join(str(x) for x in ClANrs)
    DSUdata["ClANrs"] = repr(ClANrs)
    DSUdata["Con1"] = prev_context
    DSUdata["Con2"] = context
    DSUdata["QFClANrs"] = input("Clause_atoms that are indeed QF (type comma-separated numbers): ") or ""
    DSUdata["QFverb"],DSUdata["QFverbspeech"],DSUdata["QFLMR"] = verbs(DSUdata["QFClANrs"])
    DSUdata["QFSp"] = input("QFSp: ") or None
    DSUdata["QFSptype"] = input("QFSptype (1=Noun, 2=Noun_phrase, 3=Name, 4=Name_phrase, 5=Pers_pronoun, 6=Other_pronoun, 7=Suffix, 8=Verb): ") or None
    DSUdata["MySp"] = repr(input("MySp: ").split(",")) or None
    DSUdata["QFAd"] = input("QFAd: ") or None
    DSUdata["QFAdtype"] = input("QFAdtype (1=Noun, 2=Noun_phrase, 3=Name, 4=Name_phrase, 5=Pers_pronoun, 6=Other_pronoun, 7=Suffix, 8=Verb): ") or None
    DSUdata["QFAdprep"] = input("QFAdprep: ") or None
    DSUdata["MyAd"] = repr(input("MyAd: ").split(",")) or None
    DSUdata["QFplus"] = input("QFplus: ") or None
    DSUdata["QFplustype"] = input("QFplustype: ") or None
    DSUdata["Start_node"] = ClANrs[0]
    DSUdata["DSUtag"] = str(DSUdata["Start_node"]) + str(DSUdata["Level"])
    if DSUdata["QFClANrs"] != "":
        DSUdata["QF"] = "Yes"
    elif DSUdata["QFClANrs"] == "":
        DSUdata["QF"] = "No"
    insert_dict_in_db(c, "gcdsu", DSUdata)

#}}}
\end{python}

\begin{python}
#{{{ Main function
def main_loop():
    ClANrs =       [[],[],[],[],[],[],[],[]]
    potential_QF = [[],[],[],[],[],[],[],[]]
    pot_QF = []
    context = ""
    prev_context = ["","","","","","","",""]

    place = None
    places =       ["","","","","","","",""]
    level = 0
    completed = False
    check_node = ""

    for node in NN():
        otype = F.etcbc4_db_otype.v(node)

        if otype == "book" and F.etcbc4_sft_book.v(node) != "Genesis":
            break
        elif otype == "chapter":
            if int(F.etcbc4_sft_chapter.v(node)) < 2:    # Prevent re-doing already completed chapters
                completed = True
            elif int(F.etcbc4_sft_chapter.v(node)) > 3:    # Set a limit on the amount of data dealt with this time
                break
            else:
                completed = False

        elif otype == "verse" and completed == False:
            place = F.etcbc4_sft_label.v(node)
        elif otype == "clause" and completed == False:
            prev_level = level
            level = F.etcbc4_ft_txt.v(node).count("Q")      # Determine level by parsing text_type data
            previous_context = context
            context = F.etcbc4_ft_txt.v(node)

        elif otype == "clause_atom" and completed == False:
            pot_QF.append(node)     # Update pot_QF
            if len(pot_QF) > 5:
                del pot_QF[0]       # Keep length of pot_QF reasonable
            if level > 0:           # At least one DSU under construction
                for i in range(1,(level+1)):
                    ClANrs[level].append(node)   # For all DSU's under construction, add current ClANr to list of ClANrs
            if prev_level < level:                  # Start of new DSU
                places[level] = place               # Store starting place of new DSU
                potential_QF[level] = pot_QF[:-1]   # Store potential QF of new DSU
                prev_context[level] = previous_context    # Store context (preceding text_type)

            elif prev_level > level:                # End of one or more DSU's
                for j in range((level+1),(prev_level+1)): # Process the DSU's that end here
                    check_node = str(ClANrs[j][0])
                    cursor.execute("SELECT EXISTS (SELECT * FROM gcdsu WHERE Start_node = " + check_node + ");") # Check whether DSU is not yet saved in database
                    result = cursor.fetchone()
                    if result == (0,):
                        process_DSU(ClANrs[j], potential_QF[j],places[j],prev_context[j],context,j)
                    ClANrs[j], potential_QF[j], places[j], prev_context[j] = [],[],"",""    # Delete all stored data

#}}}
\end{python}

\begin{python}
#{{{ Execute main loop
main_loop()
#}}}

#{{{ Wrap up the database connection
db.commit()
c.close()
#}}}
\end{python}

\chapter{encoding\_options.py}
\chapter{Part-1.py}
\begin{python}
# Setup the database stuff
import sqlite3

db = sqlite3.connect('gcdata.sqlite')
cursor = db.cursor()

#create_table_query = open("pt.sql").read()
#cursor.execute(create_table_query)

db.commit()

\end{python}

\begin{python}
#{{{ Find fitting info about types and functions
    function = F.etcbc4_ft_function.v(Phrase)
    if Data["Nodetype"] in ("1", "2", "3"):    # F.etcbc4_ft_function.v(node) only applies to phrase level (or lower)
        if function in ("ExsS","IntS","ModS","NCoS","PrcS","PreS") and Data["Nodetype"] == "1":
            Data["Role"] = "Subject"
        elif function in ("PreO","PtcO") and Data["Nodetype"] == "1":
            Data["Role"] = "Object"
        elif Data["Nodetype"] == "1" or F.etcbc4_ft_rela.v(SPhNr) == "rec":
            Data["Role"] = input("Grammatical role / function:" + "rec") or "rec"
        else:
            Data["Role"] = input("Grammatical role / function: " + function ) or function
    else:
        Data["Role"] = input("Grammatical role / function: ")

#}}}
\end{python}

\begin{python}
# Propose some data about a participant if this participant is recognized by gcdata.pt
def recognize(Des):
    Recognize = list(cursor.execute("select Name,Anim,Human,Col from pt where Des=:Des", {"Des":Des, }))
    if Recognize == []:
        return []
    else:
        return Recognize

\end{python}

\begin{python}
# Prepare references for interactive procedure by collecting some data
def ask_potential_ref(place, candidate, node_type, sub):
    cand = candidate
    if str(candidate).endswith("s") or str(candidate).endswith("c") or str(candidate).endswith("d"):
        cand = int(candidate[:-1])
        print("This was a suffix called ",candidate, "cand is ",cand)
    Recognize = []
    if node_type == "word+": 
        data[candidate]= {}
        data[candidate]["Des"] = get_cons_of_word_list(give_me_your_words(cand))
        data[candidate]["a"] = place
        data[candidate]["Node"] = candidate
        nodetype = F.etcbc4_db_otype.v(cand)
        if nodetype == "word":
            data[candidate]["Nodetype"] = "2"
        elif nodetype in ("subphrase", "phrase", "phrase_atom"):
            data[candidate]["Nodetype"] = "3"
        elif nodetype in ("clause","clause_atom"):
            data[candidate]["Nodetype"] = "4"
        if not get_cons_of_word_list(give_me_your_words(cand)) == "KL":
            Recognize = recognize(get_cons_of_word_list(give_me_your_words(cand)))
        if not Recognize == []:
            data[candidate]["Name"] = Recognize[0][0]
            data[candidate]["Anim"] = Recognize[0][1]
            data[candidate]["Human"] = Recognize[0][2]
            data[candidate]["Col"] = Recognize[0][3]
        data[candidate]["Sub"] = sub
    else:
        print("Treated as a suffix")
        data[candidate] = {}
        data[candidate]["Des"] = F.etcbc4_ft_prs.v(cand)
        data[candidate]["Nodetype"] = "1"
        data[candidate]["a"] = place
        data[candidate]["Node"] = int(cand)
        data[candidate]["Destype"] = "7"
        data[candidate]["Sub"] = sub
    return(data[candidate])

\end{python}

\begin{python}
# Automatically add data to entries that are going to be fed to gcdata.pt
def add_data(data,candidate):
#    cand = candidate
#    if str(candidate).endswith("s") or str(candidate).endswith("c") or str(candidate).endwith("d"):
#        cand = candidate[:-1]
    ClANr = L.u('clause',data[candidate]["Node"])
    data[candidate]["ClANr"] = ClANr
    data[candidate]["Emb"] = F.etcbc4_ft_tab.v(ClANr)
    if data[candidate]["Nodetype"] in ("4","5"):
        data[candidate]["PhANr"] = 00
    else:
        data[candidate]["PhANr"] = L.u('phrase',data[candidate]["Node"])
    if data[candidate]["Nodetype"] in ("3","4","5"):
        data[candidate]["SPhNr"] = 00
    else:
        data[candidate]["SPhNr"] = L.u('subphrase',data[candidate]["Node"])
    if data[candidate]["Nodetype"] in ("1","2"):
        data[candidate]["WNr"] = F.etcbc4_ft_number.v(data[candidate]["Node"])
    else:
        data[candidate]["WNr"] = 00
    if data[candidate]["Nodetype"] == "2":
        data[candidate]["P"] = F.etcbc4_ft_ps.v(data[candidate]["Node"])
        data[candidate]["N"] = F.etcbc4_ft_nu.v(data[candidate]["Node"])
        data[candidate]["G"] = F.etcbc4_ft_gn.v(data[candidate]["Node"])
        data[candidate]["Lexset"] = F.etcbc4_ft_ls.v(data[candidate]["Node"])
        data[candidate]["PartSpeech"] = F.etcbc4_ft_sp.v(data[candidate]["Node"])
    elif data[candidate]["Nodetype"] == "1":
        data[candidate]["P"] = suffix_dict[F.etcbc4_ft_prs.v(data[candidate]["Node"])]['ps']    # Since the suffix_features are not yet available in the ETCBC database, we extract them from a dictionary based on surface consonants
        data[candidate]["N"] = suffix_dict[F.etcbc4_ft_prs.v(data[candidate]["Node"])]['nu']
        data[candidate]["G"] = suffix_dict[F.etcbc4_ft_prs.v(data[candidate]["Node"])]['gn']
        data[candidate]["Lexset"] = "suffix"
        if F.etcbc4_ft_function.v(L.u('phrase',data[candidate]["Node"])) in ("ExsS","IntS","ModS","NCoS","PrcS","PreS"):
            data[candidate]["Role"] = "Subject"
        elif F.etcbc4_ft_function.v(L.u('phrase',data[candidate]["Node"])) in ("PreO","PtcO"):
            data[candidate]["Role"] = "Object"
    data[candidate]["Tag"] = data[candidate]["a"].rjust(15,' ') + "_" + str(data[candidate]["ClANr"]).rjust(9,'0') + "_" + str(data[candidate]["PhANr"]).rjust(9,'0') + "_" + str(data[candidate]["WNr"]).rjust(9,'0') + "_" + str(data[candidate]["Nodetype"])
    return(data[candidate])

\end{python}

\begin{python}
def interactive_single(data,candidate):
    print(data[candidate]["a"])
    print("clause:", get_cons_of_word_list(give_me_your_words(L.u('clause',data[candidate]["Node"])))) 
    print("suffix:", data[candidate]["Des"])
    data[candidate]["Sub"] = input("Is this a sub-participant? Default 'Prep' ") or "Prep" 
    data[candidate]["Name"] = input("What is the name of the referent? : ")
    data[candidate]["Anim"] = input("1=Animate, 2=Non-animate: ")
    if data[candidate]["Anim"] == "2":
        data[candidate]["Human"] = "None"
    else:
        data[candidate]["Human"] = input("1=Human (or divine), 2=Non-human, 3=Humans included: ")
    data[candidate]["Col"] = input("Is the participant an individual (1), collective (2) or an individual in a compound design (3)? Or dual/plural?(4)")
    if data[candidate]["Col"] in ("2","3","4"):
        data[candidate]["Referents"] = input("Contextually relevant referents that are part of Name: ")
        data[candidate]["ColPart"] = input("Contextually relevant collective of which Name is part: ") or None
    else:
        data[candidate]["Referents"] = None
        data[candidate]["ColPart"] = input("Contextually relevant collective of which Name is part: ") or None
    insert_dict_in_db(cursor,"pt",data[candidate])

\end{python}

\begin{python}
def interactive_clan(data,clan_members):
    print(data[clan_members[-1]]["a"], " Clause:", get_cons_of_word_list(give_me_your_words(L.u('clause',clan_members[-1]))))
#    print("Nodetypes: 1=Part_suffix, 2=Part_word, 3=Part_(sub)phrase, 4=Part_clause, 5=Part_sentence")
#    print("Destypes: 1=Noun, 2=Noun_phrase, 3=Name, 4=Name_phrase, 5=Pers_pronoun, 6=Demonstr_pronoun, 7=Suffix, 8=Verb, 9=Adjective")
#    print("Anim: 1=Animate, 2-Non-animate; Human: 1=Human, 2=Non-human, 3=Humans included")
#    print("Col: 1=Individual, 2=Collective, 3=Individual in compound design, 4=Plural")
    for candidate in clan_members:
        for x in ["Nodetype","Destype","Name","Anim","Human","Col"]:
            if not x in data[candidate]:
                data[candidate][x] = None
        if F.etcbc4_db_otype.v(candidate) == "word" and not data[candidate]["Nodetype"] == "1":
            data[candidate]["Nodetype"] = "2"
        if data[candidate]["Destype"] == "1":
            data[candidate]["Nodetype"] = "2"
        if data[candidate]["Anim"] == "2":
            data[candidate]["Human"] = "2"
        print("{0!s: <9} {1!s: <15} {2!s: <40}".format(data[candidate]["Node"], data[candidate]["Sub"], data[candidate]["Des"]))
        print("********* Nodetype: {0!s: <4}, Destype: {1!s: <4}, Name: {2!s: <10}, Anim: {3!s: <4}, Human: {4!s: <4}, Col: {5!s: <4}".format(data[candidate]["Nodetype"], data[candidate]["Destype"], data[candidate]["Name"], data[candidate]["Anim"], data[candidate]["Human"], data[candidate]["Col"]))
    clan_members2 = list(clan_members)
    for candidate in clan_members:
        copy = input("Do you need a copy of reference " + str(candidate) + "? Default 'N' ") or "N"
        if copy == "Y":
            copy_ref = str(candidate)+"c"
            clan_members2.append(copy_ref)
            data[copy_ref] = deepcopy(data[candidate])
            copy2 = input("Do you need another copy of reference " + str(candidate) + "? Default 'N' ") or "N"
            if copy2 == "Y":
                copy2_ref = str(candidate)+"d"
                clan_members2.append(copy2_ref)
                data[copy2_ref] = deepcopy(data[candidate])
        elif copy == "X":
            data.pop(candidate, None)
            clan_members2.remove(candidate)
            if clan_members2 == []:
                return
    clan_members3 = list(clan_members2)
    for candidate in clan_members2:
        for x in ["Nodetype","Destype","Name","Anim","Human","Col"]:
            if data[candidate][x] in (None,"None"):
                data[candidate][x] = input(str(x) + " of node " + str(candidate) + ": ")
                if data[candidate]["Destype"] == "X":
                    data.pop(candidate, None)
                    clan_members3.remove(candidate)
                    if clan_members3 == []:
                        return
    for candidate in clan_members3:
        print("{0!s: <9} {1!s: <15} {2!s: <40}".format(candidate, data[candidate]["Sub"], data[candidate]["Des"]))
        print("********* Nodetype: {0!s: <4}, Destype: {1!s: <4}, Name: {2!s: <10}, Anim: {3!s: <4}, Human: {4!s: <4}, Col: {5!s: <4}".format(data[candidate]["Nodetype"], data[candidate]["Destype"], data[candidate]["Name"], data[candidate]["Anim"], data[candidate]["Human"], data[candidate]["Col"]))
    for candidate in clan_members3:
        check = input("Is the data of " + str(candidate) + " correct? Default 'Y' ") or "Y"
        while check != "Y":
            for x in ("Des","Sub","Nodetype","Destype","Name","Anim","Human","Col"):
                data[candidate][x] = input(x + " of node " + str(candidate) + ", default = " + str(data[candidate][x]) + " ") or data[candidate][x] 
                if data[candidate]["Destype"] == "X":
                    data.pop(candidate, None)
            print("{0!s: <9} {1!s: <15} {2!s: <40}".format(candidate, data[candidate]["Sub"], data[candidate]["Des"]))
            print("********* Nodetype: {0!s: <4}, Destype: {1!s: <4}, Name: {2!s: <10}, Anim: {3!s: <4}, Human: {4!s: <4}, Col: {5!s: <4}".format(data[candidate]["Nodetype"], data[candidate]["Destype"], data[candidate]["Name"], data[candidate]["Anim"], data[candidate]["Human"], data[candidate]["Col"]))
            check = input("Is the data correct? Default 'Y' ") or "Y"
        data[candidate]["Referents"] = input("Contextually relevant referents that are part of Name: ") or None
        data[candidate]["ColPart"] = input("Contextually relevant collective of which Name is part: ") or None
        if check == "Y":
            add_data(data,candidate)
            insert_dict_in_db(cursor,"pt",data[candidate])
            db.commit()

\end{python}

\begin{python}
# Let the computer create as much data as possible, especially with REG-rectum data, and do a proposal to the user.
def major_flow(place, node, node_type):
    while F.etcbc4_ft_rela.v(parent[node]) == "rec" or F.etcbc4_ft_rela.v(parent[node]) == "atr": # Regens-rectum relationship or attributive relationship
        if list(C.mother.v(parent[node])) not in ([],None):
            node = list(C.mother.v(parent[node]))[0] # Deal with the clan-head instead of a (grand)child
    clan = {}
    clan_members = []
    data = {}
    if not F.etcbc4_ft_g_cons.v(node) == get_cons_of_word_list(give_me_your_words(parent[node])):
        clan[parent[node]] = "Super "
        clan[node] = "Sub1 "
    else:
        clan[node] = None
    for candidate in give_me_your_daughters(node):
        if F.etcbc4_db_otype.v(candidate) == "subphrase" and F.etcbc4_ft_rela.v(candidate) == "rec":
            clan[candidate] = "Rec "
        elif F.etcbc4_db_otype.v(candidate) == "subphrase" and F.etcbc4_ft_rela.v(candidate) == "atr":
            clan[candidate] = "Attr "
        if len(L.d('word',candidate)) == 1 and get_cons_of_word_list(give_me_your_words(candidate)) == F.etcbc4_ft_g_cons.v(L.d('word',candidate)[0]):
            clan[L.d('word',candidate)[0]] = deepcopy(clan[candidate])
            del clan[candidate]
            candidate = L.d('word',candidate)[0] 
        for x in C.mother.v(candidate):
#            clan[x] += "Super "
            if not parent[x] in clan:
                clan[parent[x]] = "Super "
    for candidate in clan:
        clan_members.append(candidate)
        subdata = deepcopy(clan[candidate])
        data[candidate] = ask_potential_ref(place, candidate, "word+", subdata)
        if not F.etcbc4_ft_prs.v(candidate) in ("absent","n/a",None):
            candidate2 = str(candidate)+"s"
            clan_members.append(candidate2)
            data[candidate2] = ask_potential_ref(place, candidate2, "suffix", subdata)
        else:
            print("no suffix found")
#    insert_dict_in_db(cursor,"pt",data[candidate])
#    return
    interactive_clan(data,clan_members)

\end{python}

\begin{python}
# Record some statistics and filter potential participant references
DSU = False
completed = False
potential_ref = False
potential_suffix = False
data = {}

limit = 0
for node in NN():
    otype = F.etcbc4_db_otype.v(node)
    if otype == "book" and F.etcbc4_sft_book.v(node) != "Genesis":
        break
#    elif otype == "chapter":
#        if int(F.etcbc4_sft_chapter.v(node)) < 1:    # Prevent re-doing already completed chapters
#            completed = True
#        elif int(F.etcbc4_sft_chapter.v(node)) > 3:    # Set a limit on the amount of data dealt with this time
#            break
#        else:
#            completed = False 
    elif otype == "verse":
#        if int(F.etcbc4_sft_verse.v(node)) < 22:
#           completed = True
#        else:
#           completed = False
        if completed == False:
            place = F.etcbc4_sft_label.v(node)
    elif otype == "clause" and completed == False:
        DSU = (F.etcbc4_ft_txt.v(node).count("Q") > 0)    # If the text_type contains a Q, we consider the clause as Direct Speech
    elif F.etcbc4_db_otype.v(node) == "word" and DSU == True and completed == False:
        check_node = str(node)
#        if check_node != "":     # Just a hack when temporarily outcommenting lines below
        cursor.execute("SELECT EXISTS (SELECT * FROM pt WHERE Node = " + check_node + ");") # Check whether reference is not yet saved in database
        result = cursor.fetchone()
        if result == "(0,)":
            print("catch!")
        potential_ref = False
        potential_suffix = False
        if not (F.etcbc4_ft_ps.v(node) in ("unknown", "NA") and F.etcbc4_ft_nu.v(node) in ("unknown","NA") and F.etcbc4_ft_gn.v(node) in ("unknown","NA")):
            potential_ref = True
        if not F.etcbc4_ft_prs.v(node) in ("absent","n/a"):
            potential_suffix = True
        if result == (0,) and potential_ref == True: # Not yet saved, to be dealt with in major flow (may have suffixes as well, these will be parsed within the major flow)
            major_flow(place, node, "word")
        if result == (0,) and potential_ref == False and potential_suffix == True: # Not yet saved, not to be dealt with in major flow, but suffix needs to be parsed
            print("single-catch")
            candidate = str(node)+"s"
            data[candidate] = ask_potential_ref(place, node, "single_suffix", "Prep")
            data[candidate] = add_data(data,candidate)
            interactive_single(data,candidate)
#            insert_dict_in_db(cursor,"pt",data[candidate])

\end{python}

\begin{python}
#{{{ Commit and close the database
db.commit()
db.close()
#}}}
\end{python}

\chapter{Part-2.py}
\begin{python}
#{{{ Load database
import sqlite3

db = sqlite3.connect('gcdata.sqlite')
c = db.cursor()
#}}}
\end{python}

From gcdata.gcdsu we want to extract the \mi{DSU_Tag}, so that we can easily see
whether various participant references occur in the same DSU or in a different one.
For each DSU we create a dictionary item with its \mi{Tag}, \mi{place} and \mi{clause_atom_numbers}.
We then check of which DSU the \mi{clause_atom_number} of the current participant
reference is part, and store its \mi{Tag} in the column \mi{DSU}.

\begin{python}
#{{{ Assign DSU and Level
DSU_Tags = list(cursor.execute("select DSUtag,Level,ClANrs from gcdsu"))
pts = list(cursor.execute("select Tag,a,ClANr from pt where DSU is null or DSU = ''"))

DSU = {}

for (Tag,a,ClANr) in pts:
    for (DSUtag,Level,ClANrs) in DSU_Tags:
        if str(ClANr) in ClANrs.split(","):
            cursor.execute("update pt set DSU=:DSUtag,Level=:Level where a=:a and ClANr=:ClANr", {"DSUtag":DSUtag,"Level":Level,"a":a,"ClANr":str(ClANr)})
            DSU[Tag] = DSUtag

#}}}
\end{python}

\begin{python}
#{{{ Feed participants back to DSU (follows on previous block)
parts = {}

for (DSUtag,Level,ClANrs) in DSU_Tags:
    parts[DSUtag] = list(cursor.execute("select Part from gcdsu where DSUtag=:DSUtag", {"DSUtag":DSUtag, }))[0][0]

for (Tag,a,ClANr) in pts:
    probable_parts = []
    ColPart = list(cursor.execute("select ColPart from pt where Tag=:Tag", {"Tag":Tag, }))[0][0]
    if ColPart == None:
        ColPart = []
    else:
        ColPart = eval(ColPart)
    Referents = list(cursor.execute("select Referents from pt where Tag=:Tag", {"Tag":Tag, }))[0][0]
    if Referents == None:
        Referents = []
    else:
        Referents = eval(Referents)
    Name = list(cursor.execute("select Name from pt where Tag=:Tag", {"Tag":Tag, }))[0][0]
    probable_parts = ColPart + Referents
    probable_parts.append(Name)
    if parts[DSU[Tag]] != None:
        probable_parts += eval(parts[DSU[Tag]])
    all_parts = list(set(probable_parts))
    parts[DSU[Tag]] = str(all_parts)
    cursor.execute("update gcdsu set Part=:all_parts where DSUtag=:DSUtag", {"all_parts":str(all_parts), "DSUtag":DSU[Tag]})

#}}}
\end{python}

In order to facilitate the combination of information between SQlite rows,
we assign a number to each participant reference, indicating how many times
this participant has been referred to, including the current reference.
Later on, we can then identify the \mi{Ref}-values by selecting values with the
same \mi{Name} and a \mi{Nr} that is 1 lower than the current \mi{Nr}.
A difficulty arises when a participant is referred to as part of a group,
e.g. ``God said to them...'' meaning both the man and the woman of \bibleverse{Gen}[1].
Therefore I provide two kinds of information:
\begin{itemize}
\item A \mi{Nr} that indicates the number of occurrences of exactly the same referent. This is relevant for \mi{Prev}-data and \mi{Ref}-data.
\item I mention the names of relevant referents that are included in other referents etc in the column \mi{Referents}, so that these can be counted and processed later on (e.g. to see how often someone is referred to in a particular DSU).
\end{itemize}
Another aspect that adds complexity, is the occurrence of sub-participants.
I do not count them in the column \mi{Nr}, but their name is stored in the columns
\mi{Name} and \mi{Referents}.


\begin{python}
#{{{ Assign Nr - new try
nrs_todo = list(cursor.execute("select Sub, Name, Tag, Nr from pt where Nr is null or Nr = ''"))  # Select all rows that need a Nr
nrs = {}    # Create dict for all relevant Names + their Nrs

for (Sub, Name, Tag, Nr) in nrs_todo:
    if Sub == "No":
        if Name in nrs:
            newnr = nrs[Name] + 1
        elif Name not in nrs:
            oldnr = list(cursor.execute("select Nr from pt where Name=:Name ORDER BY Tag DESC LIMIT 1", {"Name":Name,}))
            if oldnr == [] or oldnr[0][0] == '':
                newnr = 1
            else:
                newnr = int(oldnr[0][0]) + 1
        nrs[Name] = newnr
        cursor.execute("update pt set Nr=:newnr where Tag=:Tag", {"newnr":newnr, "Tag":Tag})
#}}}
\end{python}

Below we do a for-loop over all rows in gcdata.pt that still miss certain data,
and update them with information about the preceding reference to this participant (\mi{Ref}),
the previous reference (irrespective of the participant) (\mi{Prev}) and the first reference
to this participant (\mi{1}).

We repeat the process for other information. Putting all these columns in one
for-loop would overload them and make the code unclear.

\begin{python}
#{{{ Update Ref, Prev and *1-data
all_tags = list(cursor.execute("select Tag from pt ORDER BY Tag"))
todo = list(cursor.execute("select Tag,Name,Nr from pt where RefNode is null or RefNode = ''"))

for (Tag,Name,Nr) in todo:
    Refdata = list(cursor.execute("select DSU, a, ClANr, PhANr, Node, Nodetype, WNr from pt where Name=:Name ORDER BY Tag DESC LIMIT 1", {"Name":Name, })) # Ref-data
    cursor.execute("update pt set RefDSU=:DSU, Refa=:a, RefClANr=:ClANr, RefPhANr=:PhANr, RefNode=:Node, RefNodetype=:Nodetype, RefWNr =:WNr where Name=:Name and Nr=:Nr",{"DSU":Refdata[0][0],"a":Refdata[0][1],"ClANr":Refdata[0][2],"PhANr":Refdata[0][3],"Node":Refdata[0][4],"Nodetype":Refdata[0][5],"WNr":Refdata[0][6],"Name":Name,"Nr":Nr}) 
    Onedata = list(cursor.execute("select DSU, a, ClANr, PhANr, Node, Nodetype, WNr from pt where Name=:Name ORDER BY Tag LIMIT 1", {"Name":Name, })) # *1-data
    cursor.execute("update pt set DSU1=:DSU, a1=:a, ClANr1=:ClANr, PhANr1=:PhANr, Node1=:Node, Nodetype1=:Nodetype, WNr1=:WNr where Name=:Name and Nr=:Nr",{"DSU":Onedata[0][0],"a":Onedata[0][1],"ClANr":Onedata[0][2],"PhANr":Onedata[0][3],"Node":Onedata[0][4],"Nodetype":Onedata[0][5],"WNr":Onedata[0][6],"Name":Name,"Nr":Nr}) 

    index = [y[0] for y in all_tags].index(Tag)  # Check the index of this row in the ordered list all_tags, in order to find the previous row = Prev_data 
    if index == 0:  # Apparently this is the first row of the table
        Prevdata = list(cursor.execute("select Name, DSU, a, ClANr, PhANr, Node, Nodetype, WNr from pt where Tag=:Tag",{"Tag":Tag, })) # Prev-data for first entry
    else:
        prev_Tag = all_tags[(index-1)][0]
        Prevdata = list(cursor.execute("select Name, DSU, a, ClANr, PhANr, Node, Nodetype, WNr from pt where Tag=:prev_Tag",{"prev_Tag":prev_Tag, })) # Prev-data
    cursor.execute("update pt set PrevName =:PrevName, PrevDSU=:DSU, Preva=:a, PrevClANr=:ClANr, PrevPhANr=:PhANr, PrevNode=:Node, PrevNodetype=:Nodetype, PrevWNr =:WNr where Name=:Name and Nr=:Nr",{"PrevName":Name,"DSU":Prevdata[0][0],"a":Prevdata[0][1],"ClANr":Prevdata[0][2],"PhANr":Prevdata[0][3],"Node":Prevdata[0][4],"Nodetype":Prevdata[0][5],"WNr":Prevdata[0][6],"Name":Name,"Nr":Nr}) 

for (Tag,Name,Nr) in todo:
    Refdata = list(cursor.execute("select Con, Level, Emb, Des, Destype, Role, P, N, G, Lexset, PartSpeech from pt where Name=:Name ORDER BY Tag DESC LIMIT 1", {"Name":Name, })) # Ref-data
    cursor.execute("update pt set RefCon=:Con, RefLevel=:Level, RefEmb=:Emb, RefDes=:Des, RefDestype=:Destype, RefRole=:Role, RefP=:P, RefN=:N, RefG=:G, RefLexset=:Lexset, RefPartSpeech=:PartSpeech where Name=:Name", {"Con":Refdata[0][0],"Level":Refdata[0][1],"Emb":Refdata[0][2],"Des":Refdata[0][3],"Destype":Refdata[0][4],"Role":Refdata[0][5],"P":Refdata[0][6],"N":Refdata[0][7],"G":Refdata[0][8],"Lexset":Refdata[0][9],"PartSpeech":Refdata[0][10],"Name":Name})
    index = [y[0] for y in all_tags].index(Tag)  # Check the index of this row in the ordered list all_tags, in order to find the previous row = Prev_data 
    if index == 0:  # Apparently this is the first row of the table
        Prevdata = list(cursor.execute("select Con, Level, Emb, Des, Destype, Role, P, N, G, Lexset, PartSpeech from pt where Tag=:Tag",{"Tag":Tag, })) # Prev-data for first entry
    else:
        prev_Tag = all_tags[(index-1)][0]
        Prevdata = list(cursor.execute("select Con, Level, Emb, Des, Destype, Role, P, N, G, Lexset, PartSpeech from pt where Tag=:prev_Tag",{"prev_Tag":prev_Tag, })) # Prev-data
    cursor.execute("update pt set PrevCon=:Con, PrevLevel=:Level, PrevEmb=:Emb, PrevDes=:Des, PrevDestype=:Destype, PrevRole=:Role, PrevP=:P, PrevN=:N, PrevG=:G, PrevLexset=:Lexset, PrevPartSpeech=:PartSpeech where Tag=:Tag", {"Con":Prevdata[0][0],"Level":Prevdata[0][1],"Emb":Prevdata[0][2],"Des":Prevdata[0][3],"Destype":Prevdata[0][4],"Role":Prevdata[0][5],"P":Prevdata[0][6],"N":Prevdata[0][7],"G":Prevdata[0][8],"Lexset":Prevdata[0][9],"PartSpeech":Prevdata[0][10],"Tag":Tag})
#}}}
\end{python}

\end{document}
%sagemathcloud={"latex_command":"pdflatex -synctex=1 -shell-escape -interact=nonstopmode 'workflow.tex'"}
